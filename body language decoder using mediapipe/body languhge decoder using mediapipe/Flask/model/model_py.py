# -*- coding: utf-8 -*-
"""model.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gvIAnfSCRqyc8WybAK5g7d_mr4pxrdF7
"""

import pandas as pd
import cv2
from sklearn.model_selection import train_test_split

!pip install mediapipe opencv-python pandas scikit-learn streamlit

import mediapipe as mp
import cv2

mp_drawing = mp.solutions.drawing_utils
mp_holistic = mp.solutions.holistic



df = pd.read_csv('/content/drive/MyDrive/coords.csv')

df.head()

df.tail()

x = df.iloc[: , 1:]
y = df.iloc[: , 0]

X_train , X_test , y_train , y_test = train_test_split(x , y , test_size = 0.2 , random_state=0)

len(X_train) , len(X_test)

X = df.drop('class', axis=1)
y = df['class']



y.value_counts().plot(kind='bar')

x.value_counts().plot(kind='bar')

from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

pipelines = {
    'lr':make_pipeline(StandardScaler(), LogisticRegression()),
    'rc':make_pipeline(StandardScaler(), RidgeClassifier()),
    'rf':make_pipeline(StandardScaler(), RandomForestClassifier()),
    'gb':make_pipeline(StandardScaler(), GradientBoostingClassifier()),
}

list(pipelines.values())[0]

import pandas as pd

# Convert non-numeric columns in X_train to numeric format
X_train = X_train.apply(pd.to_numeric, errors='coerce')

# Check if there are any NaN values after conversion
print("NaN values in X_train:", X_train.isna().sum().sum())

X_train = X_train.fillna(X_train.mean())

X_train = X_train.dropna()





fit_models = {}
for algo , pipeline in pipelines.items():
    model = pipeline.fit(X_train , y_train)
    fit_models[algo] = model

from sklearn.metrics import accuracy_score
import pickle

import pandas as pd

# Convert non-numeric columns in X_train to numeric format
X_test = X_test.apply(pd.to_numeric, errors='coerce')

# Check if there are any NaN values after conversion
print("NaN values in X_test:", X_test.isna().sum().sum())

X_test = X_test.fillna(X_test.mean())

X_test = X_test.dropna()

for algo , model in fit_models.items():
    pred = model.predict(X_test)
    print(algo , accuracy_score(y_test , pred))

with open('body_language.pkl' , 'wb') as f:
    pickle.dump(fit_models['rf'], f)

with open('body_language.pkl','rb') as f:
    model = pickle.load(f)

cap = cv2.VideoCapture(0)

# initialize the model
with mp_holistic.Holistic(min_detection_confidence=0.5,min_tracking_confidence=0.5) as holistic:

    while cap.isOpened():
        ret , frame = cap.read()

        image = cv2.cvtColor(frame , cv2.COLOR_BGR2RGB)

        image.flags.writeable = False

        results = holistic.process(image)

        # print(results.pose_landmarks)
        image.flags.writeable = True

        image = cv2.cvtColor(image , cv2.COLOR_RGB2BGR)

        # face
        mp_drawing.draw_landmarks(image , results.face_landmarks , mp_holistic.FACE_CONNECTIONS,
        mp_drawing.DrawingSpec(color=(80,110,10) ,thickness = 1 ,circle_radius=1 ) ,         mp_drawing.DrawingSpec(color=(80,256,121) ,thickness = 1 ,circle_radius=1 ))

        # right hand
        mp_drawing.draw_landmarks(image , results.right_hand_landmarks , mp_holistic.HAND_CONNECTIONS,
        mp_drawing.DrawingSpec(color=(80,22,10) ,thickness = 2 ,circle_radius=4 ) ,          mp_drawing.DrawingSpec(color=(80,44,121) ,thickness = 2 ,circle_radius=2 ) )

        # left hand
        mp_drawing.draw_landmarks(image , results.left_hand_landmarks , mp_holistic.HAND_CONNECTIONS,
        mp_drawing.DrawingSpec(color=(121,22,76) ,thickness = 2 ,circle_radius=4 ) ,         mp_drawing.DrawingSpec(color=(121,44,230) ,thickness = 2 ,circle_radius=2 ))

        # pose detection
        mp_drawing.draw_landmarks(image , results.pose_landmarks , mp_holistic.POSE_CONNECTIONS,
        mp_drawing.DrawingSpec(color=(245,117,66) ,thickness = 2 ,circle_radius=4 ) ,        mp_drawing.DrawingSpec(color=(245,66,230) ,thickness = 2 ,circle_radius=2 ))

        # export coordinates
        try:
            pose = results.pose_landmarks.landmark
            pose_row = list(np.array([[landmark.x , landmark.y , landmark.z , landmark.visibility] for landmark in pose]).flatten())

            face = results.face_landmarks.landmark
            face_row = list(np.array([[landmark.x , landmark.y , landmark.z , landmark.visibility] for landmark in face]).flatten())

            row = pose_row + face_row
            X = pd.DataFrame([row])
            body_language_class = model.predict(X)[0]
            body_language_prob = model.predict_proba(X)[0]
            # print(body_language_class , body_language_prob)

            # grab ear coords
            coords = tuple(
                np.multiply(
                    np.array(
                        (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x ,                                       results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y)) ,
                        [640,480]).astype(int)
                        )

            cv2.rectangle(image,
            (coords[0],coords[1]+5) ,
            (coords[0]+len(body_language_class)*20,coords[1]-30), (245 , 117 , 16),-1)

            cv2.putText(image , body_language_class , coords ,
             cv2.FONT_HERSHEY_SIMPLEX , 1 , (255,255,255),2 , cv2.LINE_AA)

        except:
            pass

        cv2.imshow("Holistic Model Detections",image)

        if cv2.waitKey(30) & 0xFF == ord('q'):
            break

cap.release()
cv2.destroyAllWindows()

import streamlit as st
import pandas as pd
import tensorflow as tf
import mediapipe as mp
import cv2
import numpy as np
import pickle
import csv
import os

st.cache(allow_output_mutation=True)
with open(r'/content/body_language.pkl', 'rb') as f:
    model = pickle.load(f)

st.write("""# Body Language Detection""")

mp_drawing = mp.solutions.drawing_utils  # Drawing Helpers
mp_holistic = mp.solutions.holistic  # Mediapipe Solutions

run = st.checkbox('Run')
FRAME_WINDOW = st.image([])

camera = cv2.VideoCapture(0)

while run:
    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
        while camera.isOpened():
            ret, frame = camera.read()

            # Recolor Feed image
            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            image.flags.writeable = False

            # Make Detections
            results = holistic.process(image)

            # Recolor image back to BGR for rendering
            image.flags.writeable = True
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

            # 1. Draw face Landmarks
            mp_drawing.draw_landmarks(
                image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,
                mp_drawing.DrawingSpec(color=(88, 110, 10), thickness=1, circle_radius=1),
                mp_drawing.DrawingSpec(color=(80, 256, 121), thickness=1, circle_radius=1)
            )

            # 2. Right hand
            mp_drawing.draw_landmarks(
                image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
                mp_drawing.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=4),
                mp_drawing.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2)
            )

            # 3. Left Hand
            mp_drawing.draw_landmarks(
                image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
                mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),
                mp_drawing.DrawingSpec(color=(121, 44, 258), thickness=2, circle_radius=2)
            )

            # 4. Pose Detections
            mp_drawing.draw_landmarks(
                image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,
                mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),
                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)
            )

            # Export coordinates
            try:
                pose = results.pose_landmarks.landmark
                pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in pose]).flatten())

                face = results.face_landmarks.landmark
                face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in face]).flatten())

                row = pose_row + face_row
                X = pd.DataFrame([row])
                body_language_class = model.predict(X)[0]
                body_language_prob = model.predict_proba(X)[0]

                # Grab ear coords
                coords = tuple(np.multiply(np.array((
                    results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x,
                    results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y)), [648, 488]).astype(int))

                cv2.rectangle(image, (coords[0], coords[1] + 5),
                              (coords[0] + len(body_language_class) * 20, coords[1] - 30), (245, 117, 16), -1)

                cv2.putText(image, body_language_class, coords,
                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)

            except Exception as e:
                print(e)

            # Get status box.
            cv2.rectangle(image, (0, 0), (250, 60), (245, 117, 16), -1)

            # Display Class
            cv2.putText(image, 'CLASS', (95, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)
            cv2.putText(image, body_language_class.split(" ")[0], (90, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)

            # Display Probability
            cv2.putText(image, 'PROB', (15, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)
            cv2.putText(image, str(round(body_language_prob[np.argmax(body_language_prob)], 2)),
                        (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)

            FRAME_WINDOW.image(image)

            if cv2.waitKey(10) & 0xFF == ord('Q'):
                break

        camera.release()
        cv2.destroyAllWindows()
else:
    st.write('Stopped')